{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/kaggle/input/datasetSecure/data.json', 'r') as file:\n",
    "    datasec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=datasec[\"token\"]\n",
    "command = f\"huggingface-cli login --token {token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -U\n",
    "!pip install transformers accelerate git+https://github.com/kashif/diffusers.git@a3dc21385b7386beb3dab3a9845962ede6765887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "model1 = \"\"\n",
    "# model = \"soundarya2873/llama-2-7b-chat-finetuned1\"\n",
    "# model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model=\"alapatirohith/llama-2-7b-chat-finetuned1_\"\n",
    "# model=\"Dhanunjaysuppa/llama-2-7b-chat-finetuned1_fun\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"6617f01ba634747a22e3ab3a\"\n",
    "text = \"\"\"KESHAV MEMORIAL INSTITUTE OF TECHNOLOGY\n",
    "(AN AUTONOMOUS INSTITUTION)\n",
    "Narayanguda, Hyderabad, Telangana 500029\n",
    "B.Tech 2 Year 1 Sem KR21 Regular MARCH-2023 Results\n",
    "Hall Ticket: Enter HallTicket Number\n",
    "Date of\n",
    "Birth:DD/MM/YYYY\n",
    "Show Results      Clear\n",
    "KESHAV MEMORIAL INSTITUTE OF TECHNOLOGY\n",
    "(AN AUTONOMOUS INSTITUTE)\n",
    "Narayanguda, Hyderabad, Telangana 500029\n",
    "B.Tech 2 Year 1 Sem KR21 Regular MARCH-2023 Result\n",
    "Name SUPPA DHANUNJAYA Hall Ticket 21BD1A1255 Branch IT\n",
    "S.NO Subject Code SubjectsGrade\n",
    "SecuredGrade\n",
    "Points ResultCredits\n",
    "1 21301 Machine Learning Using Python Lab O 10 P 1\n",
    "2 21302 Data Structures Through C++ Lab O 10 P 2\n",
    "3 21303 Analog and Digital Electronics Lab A+ 9 P 1\n",
    "4 21304 Gender Sensitization Lab O 10 P 0\n",
    "5 213AA Mathematical Foundations of Computer\n",
    "ScienceA 8 P 4\n",
    "6 213AB Computer Organization and Architecture B 6 P 3\n",
    "7 213AC Data Structures Through C++ F 0 F 0\n",
    "8 213AD Analog and Digital Electronics B 6 P 3\n",
    "9 213AE Introduction to Machine Learning B 6 P 4\n",
    "*Note: -2 Indicates\n",
    "WithheldREGISTERED : 9 APPEARED : 9 PASSED : 8 TOTAL 18\n",
    "Result : FAIL Semester Grade Point Average(SGPA) : -Print\n",
    "Copyright ï¿½ 2023 Teleparadigm Networks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_clusters=math.ceil(len(text)/1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l=text.split(\". \")\n",
    "import re\n",
    "l=re.split(r'\\. |\\n',text)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<len(l):\n",
    "  if len(l[i])<=1:\n",
    "    l.remove(l[i])\n",
    "  else:\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import textdistance  # Make sure to install this library using: pip install textdistance\n",
    "\n",
    "# Sample string l\n",
    "# l = [\"apple\", \"banana\", \"orange\", \"grape\", \"cherry\", \"pineapple\", \"blueberry\", \"strawberry\"]\n",
    "\n",
    "# Function to calculate Levenshtein distance between two strings\n",
    "def levenshtein_distance(str1, str2):\n",
    "    return textdistance.levenshtein.normalized_distance(str1, str2)\n",
    "\n",
    "# Convert string l to a matrix of token counts\n",
    "# vectorizer = TfidfVectorizer(analyzer='char', use_idf=False)\n",
    "# X = vectorizer.fit_transform(l).toarray()\n",
    "\n",
    "# Calculate pairwise Levenshtein distances\n",
    "distances = np.zeros((len(l), len(l)))\n",
    "for i in range(len(l)):\n",
    "    for j in range(i+1, len(l)):\n",
    "        distances[i, j] = distances[j, i] = levenshtein_distance(l[i], l[j])\n",
    "\n",
    "# Perform k-means clustering\n",
    "# num_clusters = 6  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(distances)\n",
    "\n",
    "# Add cluster labels to the original l\n",
    "cluster_labels = kmeans.labels_\n",
    "# result = pd.DataFrame({'String': l, 'Cluster': cluster_labels})\n",
    "# m=max(cluster_labels)+1\n",
    "# d={}\n",
    "# d2={}\n",
    "# for i in range(len(cluster_labels)):\n",
    "#   x=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "#   if(len(x)>1200):\n",
    "#     if(cluster_labels[i] in d2):\n",
    "#       d[d2[cluster_labels[i]]]=d[d2[cluster_labels[i]]]+l[i]+\". \"\n",
    "#     else:\n",
    "#       d2[cluster_labels[i]]=m\n",
    "#       d[m]=l[i]+\". \"\n",
    "#       m+=1\n",
    "#   else:\n",
    "#     d[cluster_labels[i]]=x\n",
    "\n",
    "d={}\n",
    "for i in range(len(cluster_labels)):\n",
    "  d[cluster_labels[i]]=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "\n",
    "# Display the result\n",
    "# print(result)\n",
    "l2=list(d.values())\n",
    "def split_long_strings(strings, max_length):\n",
    "    result = []\n",
    "    for string in strings:\n",
    "        if len(string) > max_length:\n",
    "            # Split the long string into parts of max_length\n",
    "            # parts = [string[i:i+max_length] for i in range(0, len(string), max_length)]\n",
    "            strs=string.split(\". \")\n",
    "            str1=strs[0]\n",
    "            for i in range(1,len(strs)):\n",
    "                if(len(str1+strs[i])<=max_length):\n",
    "                    str1+=strs[i]\n",
    "                else:\n",
    "                    result.append(str1)\n",
    "                    str1=strs[i]\n",
    "            result.append(str1)\n",
    "        else:\n",
    "            result.append(string)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "cluster_list = split_long_strings(l2,1200)\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "def generate_summary(text_chunk):\n",
    "    # Defining the template to generate summary\n",
    "    template = \"\"\"\n",
    "    Context:\n",
    "    You are tasked with summarizing a large body of text that has been divided into clusters for efficient processing. Each cluster contains related information, and the goal is to generate a concise and informative summary for each cluster.\n",
    "    Input text:\n",
    "    ```{text}```\n",
    "    SUMMARY:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    summary = llm_chain.run(text_chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymongo\n",
    "!/opt/conda/bin/python3.10 -m pip install \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "connection_string=datasec[\"data\"]\n",
    "client = MongoClient(connection_string)\n",
    "db = client.Llama2\n",
    "collection = db.summary\n",
    "query_criteria = {\"_id\": ObjectId(id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# chunk_summaries = []\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "\n",
    "combined_summary=\"\"\n",
    "# Set the max_split_size_mb environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
    "for chunk in cluster_list:\n",
    "    summary = generate_summary(chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(summary)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"done\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Clear CUDA memory after each iteration\n",
    "    combined_summary+=summary+\"\\n\"\n",
    "    update_query = {\"$set\": {\"summary\": combined_summary}}\n",
    "    collection.update_one(query_criteria, update_query)\n",
    "    # chunk_summaries.append(summary)\n",
    "\n",
    "\n",
    "# combined_summary = \"\\n\".join(chunk_summaries)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
