{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/kaggle/input/datasetSecure/data.json', 'r') as file:\n",
    "    datasec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=datasec[\"token\"]\n",
    "command = f\"huggingface-cli login --token {token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -U\n",
    "!pip install transformers accelerate git+https://github.com/kashif/diffusers.git@a3dc21385b7386beb3dab3a9845962ede6765887\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "model1 = \"\"\n",
    "# model = \"soundarya2873/llama-2-7b-chat-finetuned1\"\n",
    "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model=\"alapatirohith/llama-2-7b-chat-finetuned1_\"\n",
    "# model=\"Dhanunjaysuppa/llama-2-7b-chat-finetuned1_fun\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"6659ca2f9b46d92c60a2a082\"\n",
    "text = r\"\"\"ALAPATI ROHITH\n",
    "Professional Summary\n",
    "ProjectsSeeking a challenging opportunity in an organization which\n",
    "recognizes and utilizes my true potential while nurturing\n",
    "analytical and technical skills.\n",
    "Technical Skills\n",
    "Data Structures and Algorithms\n",
    "Object oriented programming\n",
    "Web development (HTML, CSS,\n",
    "Javascript, MERN)\n",
    "Machine learning\n",
    "MySQL\n",
    "Flask\n",
    "AWS cloud\n",
    "Git\n",
    "AREAS OF INTEREST\n",
    "Competitive coding\n",
    "Web development\n",
    "Problem solving in javaEducation Background\n",
    "Keshav Memorial Institute of Technology                               \n",
    "Btech, Information Technology               8.38 CGPA\n",
    "Graduating in 2025\n",
    "Sri Chaitanya Junior Kalasala\n",
    "Intermediate                                                  96.7%\n",
    "Completed in 2021\n",
    "Narayana High School\n",
    "Tenth                                                               10 GPA\n",
    "Completed in 2019My Contact\n",
    "rohithalapati234@gmail.com\n",
    "10-69, Ram Nagar, Gannavaram,\n",
    "Andhra Pradesh, 5211019492806108\n",
    "www.linkedin.com/in/alapati-\n",
    "rohith-a9259025b\n",
    "Llama 2 Multi-Document Summarization\n",
    "An user friendly interface to summarize and analyze\n",
    "multiple heavy text documents at one go, saving time and\n",
    "improving efficiency.\n",
    "Initially, users submit documents, which undergo initial\n",
    "screening through cosine similarity analysis. Non-similar\n",
    "documents are rejected, while similar ones trigger a\n",
    "Kaggle API call to execute a notebook. Within the\n",
    "notebook, the documents are segmented into coherent\n",
    "groups utilizing k-means clustering and a BERT model. The\n",
    "resulting clusters are fed into a fine-tuned Llama 2 model\n",
    "to generate concise summaries.\n",
    "Tech Stack:Llama 2, Kaggle API, K-means clustering, Colab,\n",
    "Qlora, React, Flask, Mongodb, BERT, Vercel,\n",
    "RenderCERTIFICATIONS\n",
    "Python for Data Science (NPTEL) -\n",
    "ELITE SILVER (TOP 1%)\n",
    "Introduction to Machine Learning\n",
    "(NPTEL)INTERNS/COURSES\n",
    "Completed a summer training & internship of 6 weeks on\n",
    "Machine Learning using Python\n",
    "Creadly Badge by AWS Educate for course completion in\n",
    "Reinforcement Learning\n",
    "Successfully completed a �GenAI: Prompt Engineering�\n",
    "course leveraging cutting-edge tools within the Google\n",
    "Cloud platform.\n",
    "EXTRACURRICULAR\n",
    "Participated in AWS DeepRacer Student�s League � India\n",
    "2022\n",
    "Participated in �HackForEt� HackathonPROGRAMMING\n",
    "LANGUAGES KNOWN\n",
    "JAVA\n",
    "PYTHON\n",
    "C, CPP\n",
    "LANGUAGES KNOWN\n",
    "English\n",
    "Telugu\n",
    "STRENGTHS\n",
    "Team management\n",
    "Self learner\n",
    "Determined\n",
    "Dedicated\n",
    "Leadership\n",
    "Time managementNew Spectrum � Portal for Innovation (In progress)\n",
    "A innovative platform where business real world problems\n",
    "can be solved by fruitful collaboration between business\n",
    "leaders and creative students\n",
    "New spectrum platform enables entrepreneurs to post\n",
    "and track their problem statements while students can\n",
    "solve/post their solution\n",
    "Tech Stack:Mongodb, Express, React, NodejsAutomated crop yield prediction\n",
    "Predicts the yield of a crop based on the attributes\n",
    "provided by the user using random forest machine\n",
    "learning algorithm\n",
    "Deployed in AWS cloud and packed with user friendly UI\n",
    "design for great user experience\n",
    "Tech Stack:AWS cloud (S3 bucket, EC2, Lambda,\n",
    "Sagemaker, API Gateway), Machine learning,\n",
    "HTML, CSS, Bootstrap, Javascript, RapidAPI,\n",
    "Flask, Nodejs, Tensorflow.js\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_clusters=math.ceil(len(text)/3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l=text.split(\". \")\n",
    "# import re\n",
    "# l=re.split(r'\\. |\\n',text)\n",
    "# i=0\n",
    "# while i<len(l):\n",
    "#   if len(l[i])<=25:\n",
    "#     l.remove(l[i])\n",
    "#   else:\n",
    "#     i=i+1\n",
    "# l\n",
    "from nltk.tokenize import sent_tokenize\n",
    "l=sent_tokenize(text)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings = bert_model.encode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import textdistance  # Make sure to install this library using: pip install textdistance\n",
    "\n",
    "# Sample string l\n",
    "# l = [\"apple\", \"banana\", \"orange\", \"grape\", \"cherry\", \"pineapple\", \"blueberry\", \"strawberry\"]\n",
    "\n",
    "# Function to calculate Levenshtein distance between two strings\n",
    "# def levenshtein_distance(str1, str2):\n",
    "#     return textdistance.levenshtein.normalized_distance(str1, str2)\n",
    "\n",
    "# Convert string l to a matrix of token counts\n",
    "# vectorizer = TfidfVectorizer(analyzer='char', use_idf=False)\n",
    "# X = vectorizer.fit_transform(l).toarray()\n",
    "\n",
    "# Calculate pairwise Levenshtein distances\n",
    "# distances = np.zeros((len(l), len(l)))\n",
    "# for i in range(len(l)):\n",
    "#     for j in range(i+1, len(l)):\n",
    "#         distances[i, j] = distances[j, i] = levenshtein_dist(l[i], l[j])\n",
    "\n",
    "distances=cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "for i in range(len(l)):\n",
    "    for j in range(len(l)):\n",
    "        distances[i][j]=1-distances[i][j]\n",
    "\n",
    "# Perform k-means clustering\n",
    "# num_clusters = 6  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(distances)\n",
    "\n",
    "# Add cluster labels to the original l\n",
    "cluster_labels = kmeans.labels_\n",
    "# result = pd.DataFrame({'String': l, 'Cluster': cluster_labels})\n",
    "# m=max(cluster_labels)+1\n",
    "# d={}\n",
    "# d2={}\n",
    "# for i in range(len(cluster_labels)):\n",
    "#   x=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "#   if(len(x)>1200):\n",
    "#     if(cluster_labels[i] in d2):\n",
    "#       d[d2[cluster_labels[i]]]=d[d2[cluster_labels[i]]]+l[i]+\". \"\n",
    "#     else:\n",
    "#       d2[cluster_labels[i]]=m\n",
    "#       d[m]=l[i]+\". \"\n",
    "#       m+=1\n",
    "#   else:\n",
    "#     d[cluster_labels[i]]=x\n",
    "\n",
    "d={}\n",
    "for i in range(len(cluster_labels)):\n",
    "  d[cluster_labels[i]]=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "\n",
    "# Display the result\n",
    "# print(result)\n",
    "l2=list(d.values())\n",
    "def split_long_strings(strings, max_length):\n",
    "    result = []\n",
    "    for string in strings:\n",
    "        if len(string) > max_length:\n",
    "            # Split the long string into parts of max_length\n",
    "            # parts = [string[i:i+max_length] for i in range(0, len(string), max_length)]\n",
    "            strs=sent_tokenize(string)\n",
    "            str1=strs[0]\n",
    "            for i in range(1,len(strs)):\n",
    "                if(len(str1+strs[i])<=max_length):\n",
    "                    str1+=strs[i]\n",
    "                else:\n",
    "                    result.append(str1)\n",
    "                    str1=strs[i]\n",
    "            result.append(str1)\n",
    "        else:\n",
    "            result.append(string)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "cluster_list = split_long_strings(l2,3500)\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "def generate_summary(text_chunk):\n",
    "    # Defining the template to generate summary\n",
    "    template = \"\"\"\n",
    "    Context:\n",
    "    Give one line summary\n",
    "    Input text:\n",
    "    ```{text}```\n",
    "    SUMMARY:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    summary = llm_chain.run(text_chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymongo\n",
    "!/opt/conda/bin/python3.10 -m pip install \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "connection_string=datasec[\"data\"]\n",
    "client = MongoClient(connection_string)\n",
    "db = client.Llama2\n",
    "collection = db.summary\n",
    "query_criteria = {\"_id\": ObjectId(id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# chunk_summaries = []\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "\n",
    "combined_summary=\"\"\n",
    "prev=\"\"\n",
    "# Set the max_split_size_mb environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
    "for chunk in cluster_list:\n",
    "    summary = generate_summary(prev+chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(summary)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"done\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Clear CUDA memory after each iteration\n",
    "    # sumlist=re.split(r'\\. |\\.\\n', summary)\n",
    "    # prev=sumlist[0][:280]+\". \"\n",
    "    # sumlist=summary.split(\"\\n\")\n",
    "    sumlist=sent_tokenize(summary)\n",
    "    prev=\"\"\n",
    "    for i in sumlist:\n",
    "        if len(prev+i)<=800:\n",
    "            prev+=i\n",
    "    # prev=sumlist[0][:800]+\". \"\n",
    "    combined_summary+=summary+\"\\n\"\n",
    "    \n",
    "    update_query = {\"$set\": {\"summary\": combined_summary}}\n",
    "    collection.update_one(query_criteria, update_query)\n",
    "    # chunk_summaries.append(summary)\n",
    "\n",
    "\n",
    "# combined_summary = \"\\n\".join(chunk_summaries)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
