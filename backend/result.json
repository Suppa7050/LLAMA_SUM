{
  "data": "KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n1 \n  Unit -III \nGPU Architectures and CUDA Introduction: The CPU, GPU system as an accelerated computational platform, \nThe GPU and the thread engine, Characteristics of GPU memory spaces, The  PCI bus: CPU to GPU data transfer \noverhead,  multi -GPU  platforms  and MPI,  Potential  benefits  of GPU  accelerated  platforms.  Introduction  to CUDA  \nProgramming:The history of high -performance computing \u2013 Technical requirements, Hello World from CUDA, \nThread hierarchy,  Vector  addition  using  CUDA,  Error  reporting in  CUDA,  Data  type support  in CUDA.  \n \nThe CPU, GPU system as an accelerated computational platform  \nA CPU, or Central Processing Unit, is the primary component of a computer that performs most of the \nprocessing inside the computer. It interprets instructions from the comp uter's memory, processes \nthem, and performs arithmetic and logical operations . \nA GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the \nprocessing of images and videos in a computer. Originally developed for rendering graphics in video \ngames and multimedia applications.  GPUs consist of thousands of smaller cores that can handle \nmultiple tasks simultaneously. This parallel architecture makes them highly efficient for tasks that can \nbe parallelized . \nTypes of G PU \n \n \n \n \nIntegrated GPUs  are built into the same chip as the central processing unit (CPU). They share system \nmemory (RAM) with the CPU and are commonly found in laptops, Ultrabook \u2019s, and budget desktop \ncomputers. The AMD integrated GPUs are called Accelerated Processing Units (A PUs). These are a \ntightly  coupled combination of the CPU and a GPU. In the AMD APU, the CPU and GPU  share  the same  \nprocessor  memory . Integrated GPUs are suitable for basic tasks like web browsing, office applications, \nand multimedia playback . \nA discrete GPU /Dedicated GPU  (Graphics Processing Unit) is a separate graphics card that is installed \non a computer's motherboard as an additional hardware component. Unlike integrated GPUs, which Integrated GPU  Discrete GPU  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n2 \n are integrated into the same chip as the CPU, discrete GPUs have their  own dedicated video memory \n(VRAM) and are designed to handle graphics -related tasks independently. Discrete GPUs offer \nsignificantly higher performance and are suitable for demanding applications such as gaming, video \nediting, 3D rendering, and profession al graphics work . \nCommunication Between CPU and GPU  \n \nFigure  9.1 Block  diagram  of GPU -accelerated  system  using  a dedicated  GPU.  The CPU  and \nGPU  each  have  their  own memory.  The CPU and  GPU communicate  over a PCI bus. \n \nComponents of GPU Accelerated System  \nCPU \u2014The main  processor  that is installed in  the socket  of the motherboard.  \n \nCPU  RAM \u2014The \u201cmemory  sticks\u201d  or dual  in-line memory  modules  (DIMMs)  containing  Dynamic  \nRandom - Access  Memory  (DRAM)  is a type of computer memory module that is used in desktop \ncomputers, servers, and workstations that are inserted into  the memory  slots  in the motherboard.  \n \nGPU \u2014A large  peripheral  card  installed  in a Peripheral  Component  Interconnect  Express  (PCIe)  slot \non the motherboard.  \n \nGPU  RAM \u2014Memory  modules  on the GPU  peripheral  card for  exclusive  use of the GPU.  \n \nPCI bus\u2014The wiring  that connects  the peripheral  cards  to the other  components  on the motherboard.  \n \nFigure 9.1 conceptually illustrated a CPU -GPU  system with a dedicated GPU. A CPU has access to its \nown memory space (CPU RAM) and is  connected  to a GPU  via a PCI bus.  It is able  to send  data  and \ninstructions  over  the PCI bus for the GPU to work with. The GPU has its own memory space, separate \nfrom the CPU memory space. In  order for work to be executed on the GPU, at some point, data must \nbe transferred from the CPU to  the GPU. When the work is complete, and the results are going to be \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n3 \n written to file, the GPU must  send data back to the CPU. The instructions  the GPU must execute are \nalso sent from CPU to GPU.  Each  one of these  transactions  is mediated  by the PCI  bus.  \n \nThe GPU and the thread engine  \nThe thread engine within a CPU manages the execution of threads, schedules tasks for processing, and \nensures efficient utilization of available resources. The graphics processor is like the ideal thread engine.  \n\uf0d8 The components of this thread engine are  \n\uf0b7 A seemingly infinite number of threads  \n\uf0b7 Zero -time  cost for switching or starting threads:  refers to the ideal scenario where the process of \ninitiating or switching between threads occurs instantaneously, without any additional \ncomputational overhead.  \n\uf0b7 Latency  hiding of memory accesses through automatic switching between work groups : Memory \nlatency occurs because accessing data from the main memory (RAM) is significantly slower \ncompared to accessing data from the CPU's cache or registers . \n \n\uf0d8 For Example here we w ill  go through a single node system with a single multiprocessor CPU \nand two GPUs  \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n4 \n \uf0b7 Fig 9.2 Simplified Block Diagram of a GPU System consisting of two compute devices each \nhaving multiple compute units and separate GPU Memory.  \nA GPU is composed of  \n\uf0b7 Compute Device :  A compute device in a GPU is a subset of the GPU that is dedicated to \ngeneral -purpose parallel processing tasks. It consists of multiple compute units  \n\uf0b7 GPU RAM  : (also known as global memory)  refers to the dedicated memory that is integrated \ninto a graphics processing unit (GPU) or graphics card . \n\uf0b7 Workload distributor : Instructions and data received from the CPU are processed by the \nworkload distributor. The distributor coordinates instruction execution and data movement \nonto and off of the Compute Units.  \n\uf0b7 Compute units (CU ): Compute units are the fundamental processing units within a compute \ndevice. Each compute unit typically consists of multiple ALUs and multiple graphics \nprocessors called processing elements (PEs) . CUs have their own internal architecture, often \nreferred to as the microarchitecture . \n \n \n\uf0b7 Processing Element :  PEs within a GPU are designed to execute instructions in parallel. They \ncan handle multiple threads and data elements s imultaneously, allowing for massive \nparallelism.  \n \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n5 \n \uf0d8 Hardware Terminology  \n \nTable 9.1 summarizes the rough equivalence of terminology, in different har dware \narchitectures.  Example CPU in Host is termed as Compute Device (OpenCL ), GPU  in (AMD \nGPU) ,GPU  (NVIDIA/CUDA) and GPU in Intel Gen11.  \n \n\uf0d8 Calculating the peak theoretical flops for some leading GPUs  \n \nFLOPS provide a measure of a computer system's processing speed, especially when dealing with \nnumerical and scientific computations. It allows researchers  and developers to compare the \nperformance of different hardware architectures and configurations.  \nThe peak theoretical flops can be calculated by taking the clock rate times the number of processors \ntimes the number of floating -point operations per cycle.  The flops per cycle accounts for the fused -\nmultiply add (FMA), which does two operations in one cycle .  \nPeak Theoretical Flops (GFlops/s) = Clock rate MHZ \u00d7 Compute Units \u00d7 Processing units \u00d7 \nFlops/cycle . \n \nCharacteristics  of GPU  memory  spaces  \n \nGPU memory spaces are essential for efficient data management and processing in parallel \ncomputing environments, particularly in the context of graphics processing and high -performance \ncomputing.  Graphics Processing Units (GPUs) have multiple memory spaces with diffe rent \ncharacteristics, each optimized for specific types of tasks . \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n6 \n  \n \nFigure 9.4 Rectangles show each component of the GPU and the memory that is at each hardware \nlevel. The host writes and reads the global and constant memory. Each of the CUs can read and \nwrite from the global memory and read from the constant memory  \nThe list of the GPU memory types and their characteristics are  as follows.  \n\uf0b7 Private  memory  (register  memory) \u2014 Private memory in the context of GPUs typically refers \nto the local memory associated with individual processing element. It is  accessible  by a single  \nProcessing Element ( PE) and only  by that PE. \n\uf0b7 Local memory \u2014Accessible to a single C ontrol  Unit and all of the P rocessing Elements  on that \nControl Unit. \n \n\uf0b7 Constant  memory \u2014Constant memory is read -only, meaning that data stored in constant \nmemory cannot be modified by the GPU kernel during execution. It is primarily designed for \nread operations and is well-suited for storing constant values, lookup tables, or other data \nthat does not change during the kernel's execution  \n\uf0b7 Global memory \u2014Memory that \u2019s located on the GPU and accessible by all of the C ontrol \nUnit\u2019s \n \nThe performance of GPU memory, also known as memory subsystem performance, is a critical \nfactor that significantly impacts the overall performance of GPU -accelerated applications.  \n \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n7 \n  GPU memory performance is influenced by Memory Bandwidth  which can be calculated by \n \n \n \n \n \n  \n \nTheoretical Calculation:  Theoretical Bandwidth  = Memory Transaction Rate(Gbps) \u00d7 Memory \nbus (bits) \u00d7 (1 byte/8 bits)  \n \nGPU Stream Bench Mark:  In the context of GPUs (Graphics Processing Units) and general \ncomputing, a benchmark refers to a standardized test or set of tests designed to measure the \nperformance of a GPU or an entire computer system. Benchmarks are used to evaluate various \naspects of a GPU's capabilities, such as computational power , memory bandwidth, and graphics \nrendering performance.  For Example, the Babel STREAM Benchmark code measures the \nbandwidth of a variety of hardware with different programming languages.  \nThe mixbench tool  was developed to draw out the differences between the performance of \ndifferent GPU devices. Using the mixbench performance tool we can ch oose the best GPU for a \nworkload . \n \n Roofline Performance Model : The Roofline Performance Model is a graphical representation used \nin high -performance computing (HPC) to analyze and visualize the performance of algorithms on a \nTheoritical \nCalculation  GPU Stream \nBenchmark  Roofline Performance \nModel  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n8 \n particular hardware architecture . The  Roofline model serves as a visual communication tool that can \nbe easily shared among team members, researchers, and stakeholders. It provides a clear and \nconcise representation of performance characteristics, making it easier to convey insights and \noptimi zation recommendations.  \n \nThe PCI bus: CPU to GPU data transfer overhead  \nThe PCI (Peripheral Component Interconnect) bus is a standard interface that connects various \nhardware devices, including GPUs (Graphics Processing Units), to a computer's motherboard.  The \ncurrent version of the PCI bus is called PCI Express (PCIe).  \n \n \n \nThe Datatransfer includes the following steps for a typical program execeution on GPU  \n1. Copy data to GPU memory  \n2. CPU instructs the GPU (kernel configuration and launching)  \n3. Data processed by many cores in parallel  \n4. Copy result back to main memory.  \nWhen transferring data from the CPU to the GPU or vice versa over the PCI bus, there can be \noverhead associated with the data transfer process. This overhead is influenced by bandwidth  and \nMemory . \n            Bandwidth of PCI                                                                                                   Memory  \n \n \n \nEstimated : \n\uf0b7 theoretical peak \nperformance \nmodel  \n\uf0b7 micro -benchmark \napplication  Calculated : \n\uf0b7 Theoretical \nbandwidth of \nthe PCI bus  Pinned \nMemory  Pageable \nMemory  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n9 \n  \nA back -of-the-envelope theoretical peak performance model:  The Theoretical Peak Performance \nModel is a concept used in high -performance computing to estimate the maximum computational \nperformance that a system or a specific hardware component can achieve under ideal conditions. \nThis model provides an upper bound on performance based on the theoretical capabilities of the \nhardware and is often used as a reference point for evaluating the efficiency of algorithms and \napplications.  \n A micro -benchmark application : A microbenchmark is a small, focused benchmark designed to \nmeasure the performance of a specific aspect of a system, component, or function. \nMicrobenchmarks are valua ble for isolating and evaluating the performance of a particular piece of \ncode or hardware, helping developers optimize and understand the efficiency of specific operations. \nThese benchmarks are often used during the development and testing phases to identi fy bottlenecks \nand make targeted improvements.  \nTheoretical bandwidth of the PCI bus (Calculated) : \nTheoretical Bandwidth (GB/s) = PCIe Lanes \u00d7 TransferRate (GT/s) \u00d7 OverheadFactor(Gb/GT) \u00d7 byte/8 \nbits \n \n\uf0b7 PCIe lanes refer to the individual data transfer paths within a PCI Express (PCIe) interface. \nEach lane is a point -to-point connection between two components, typically between a \ndevice (e.g., a graphics card, storage device) and the computer's motherboard . The concept \nof lanes is fundamental to understanding the bandwidth and data transfer capabilities of the \nPCIe . \n\uf0b7 The maximum transfer rates for each lane in a PCIe bus can directly be determined by its \ndesign generation. Generation is a specification for the required performance of the \nhardware, much like 4G is an industry standard for cell -phones. The PCI Special Interest \nGroup (PCI SIG) represents industry partners and establishes a PCIe specification that is \ncommonly referred to as generation or gen for  short . \n\uf0b7 Transmitting data across the PCI bus requires additional overhead. Generation 1 and 2 \nstandards stipulate that 10 bytes are transmitted for every 8 bytes of useful data. Starting \nwith generation 3, the transfer transmits 130 bytes for every 128 bytes of data. The overhead \nfactor is the ratio of the number of usable bytes over the total bytes transmitted . KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n10 \n  \nPCI Express (PCIe) specifications by generation  \n \nPinned memory,  also known as locked or page -locked memory, is a type of memory in a computer \nsystem that remains fixed in physical RAM and is not subject to swapping to disk by the operating \nsystem's virtual memory manager. In GPU programming, pinned memory is often used to facilitate \nfast data transfers between the CPU and GPU. Pinned memory cons umes physical RAM exclusively. \nApplications using a significant amount of pinned memory may impact overall system resources, so \ncareful consideration is needed to avoid resource exhaustion.  \nPageable memory , also known as virtual memory, is a type of memory  management strategy used \nby operating systems to efficiently handle the allocation and deallocation of memory for running \nprocesses. In a pageable memory system, the operating system divides the physical memory into \nfixed -size blocks called pages. These p ages can be dynamically moved between the computer's RAM \n(Random Access Memory) and a secondary storage device, such as a hard disk, to optimize overall \nsystem performance.  \n \nMulti -GPU platforms and MPI  \nA multi -GPU (Graphics Processing Unit) platform refers  to a system configuration that incorporates \nmore than one GPU. Multi -GPU setups are commonly used in high -performance computing, \nscientific simulations, and graphics -intensive applications to enhance computational power and \ngraphics rendering capabilities . \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n11 \n  \nTo use multiple GPUs, we have to send data from one GPU to another.  \n\uf0d8 Standard data transfer process : This has a lot of data movement and will be a major limitation \nto application performance.  \n \n1 Copy the data from the GPU to the host processor  \n a Move the data across the PCI bus to the processor  \n b Store the data in CPU DRAM memory  \n2 Send the data in an MPI message to another processor  \na Stage the data from CPU memory to the processor  \nb Move the data across the PCI bus to the network interfac e card (NIC)  \nc Store the data from the processor to CPU memory  \n3 Copy the data from the second processor to the second GPU  \n a Load the data from CPU memory to the processor  \n b Send the data across the PCI bus to the GPU  \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n12 \n \uf0d8 Optimizing the data movement between GPUs across the network : The data movement \nbypasses the CPU when moving data from one GPU to another  \n \n \n Potential benefits of GPU -accelerated platforms  \n1. Reducing time -to-solution  \nReducing time -to-solution in GPU (Graphics Processing Unit) computing  involves optimizing the \ncode, leveraging parallel processing capabilities, and making efficient use of GPU resources . \nStructure the computations to take advantage of data parallelism, where the same operation is \nperformed on multiple data elements concurrently. This aligns well with the architecture of \nGPUs, which excel at handling parallel tasks.  \n2. Reducing energy use with GPUs  \nReducing energy use with GPUs involves optimizing your GPU -accelerated applications to \nachieve computational efficiency while considering power consumption.  \nThe energy consumption for your application can be estimated using the formula  \nEnergy = (N Processors) \u00d7 (R Watts/Processor) \u00d7 (T hours)  \nAchieving a reduction in energy cost through GPU accelerator devices requires th at the \napplication expose sufficient parallelism and that the device \u2019s resources are efficiently utilized.  \n3. Reduction in cloud computing costs with GPUs  \nCloud computing services from Google and Amazon let you match your workloads to a wide range \nof compute  server types and demands.  \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n13 \n \uf0b7 If your application is memory bound, you can use a GPU that has a lower flops to -loads ratio \nat a lower cost.  \n\uf0b7 If you are more concerned with turnaround time, you can add more GPUs or CPUs.  \n\uf0b7 If your deadlines are less serious, y ou can use preemptible resources at a considerable \nreduction in cost.  \nAs the cost of computing is more visible with cloud computing services, optimizing application \u2019s \nperformance becomes a higher priority. Cloud computing has the advantage of giving you access \nto a wider variety of hardware than you can have on -site and more options to match the hardware \nto the workload.  \nWhen to use GPUs  \nGPUs are not general -purpose p rocessors. They are most appropriate when the computation \nworkload is similar to a graphics workload \u2014lots of operations that are identical.  \nThere are some areas where GPUs still do not perform well . \n\uf0b7 Lack of parallelism \u2014 \u201cWith great power comes great need for parallelism. \u201d If you don \u2019t have \nthe parallelism, GPUs can \u2019t do a lot for you. This is the first law of GPGPU programming.  \n\uf0b7 Irregular memory access \u2014CPUs also struggle with this. The massive parallelism of GPUs \nbrings no benefit to this situation.  \n\uf0b7 Dynami c memory requirements \u2014Memory allocation is done on the CPU, which severely \nlimits algorithms that require memory sizes determined on the fly.  \n\uf0b7 Recursive algorithms \u2014GPUs have limited stack memory resources, and suppliers often state \nthat recursion is not su pported.  \n",
  "summary": "KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n1 \n  Unit -III \nGPU Architectures and CUDA Introduction: The CPU, GPU system as an accelerated computational platform, \nThe GPU and the thread engine, Characteristics of GPU memory spaces, The  PCI bus: CPU to GPU data transfer \noverhead,  multi -GPU  platforms  and MPI,  Potential  benefits  of GPU  accelerated  platforms.  Introduction  to CUDA  \nProgramming:The history of high -performance computing \u2013 Technical requirements, Hello World from CUDA, \nThread hierarchy,  Vector  addition  using  CUDA,  Error  reporting in  CUDA,  Data  type support  in CUDA.  \n \nThe CPU, GPU system as an accelerated computational platform  \nA CPU, or Central Processing Unit, is the primary component of a computer that performs most of the \nprocessing inside the computer. It interprets instructions from the comp uter's memory, processes \nthem, and performs arithmetic and logical operations . \nA GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the \nprocessing of images and videos in a computer. Originally developed for rendering graphics in video \ngames and multimedia applications.  GPUs consist of thousands of smaller cores that can handle \nmultiple tasks simultaneously. This parallel architecture makes them highly efficient for tasks that can \nbe parallelized . \nTypes of G PU \n \n \n \n \nIntegrated GPUs  are built into the same chip as the central processing unit (CPU). They share system \nmemory (RAM) with the CPU and are commonly found in laptops, Ultrabook \u2019s, and budget desktop \ncomputers. The AMD integrated GPUs are called Accelerated Processing Units (A PUs). These are a \ntightly  coupled combination of the CPU and a GPU. In the AMD APU, the CPU and GPU  share  the same  \nprocessor  memory . Integrated GPUs are suitable for basic tasks like web browsing, office applications, \nand multimedia playback . \nA discrete GPU /Dedicated GPU  (Graphics Processing Unit) is a separate graphics card that is installed \non a computer's motherboard as an additional hardware component. Unlike integrated GPUs, which Integrated GPU  Discrete GPU  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n2 \n are integrated into the same chip as the CPU, discrete GPUs have their  own dedicated video memory \n(VRAM) and are designed to handle graphics -related tasks independently. Discrete GPUs offer \nsignificantly higher performance and are suitable for demanding applications such as gaming, video \nediting, 3D rendering, and profession al graphics work . \nCommunication Between CPU and GPU  \n \nFigure  9.1 Block  diagram  of GPU -accelerated  system  using  a dedicated  GPU.  The CPU  and \nGPU  each  have  their  own memory.  The CPU and  GPU communicate  over a PCI bus. \n \nComponents of GPU Accelerated System  \nCPU \u2014The main  processor  that is installed in  the socket  of the motherboard.  \n \nCPU  RAM \u2014The \u201cmemory  sticks\u201d  or dual  in-line memory  modules  (DIMMs)  containing  Dynamic  \nRandom - Access  Memory  (DRAM)  is a type of computer memory module that is used in desktop \ncomputers, servers, and workstations that are inserted into  the memory  slots  in the motherboard.  \n \nGPU \u2014A large  peripheral  card  installed  in a Peripheral  Component  Interconnect  Express  (PCIe)  slot \non the motherboard.  \n \nGPU  RAM \u2014Memory  modules  on the GPU  peripheral  card for  exclusive  use of the GPU.  \n \nPCI bus\u2014The wiring  that connects  the peripheral  cards  to the other  components  on the motherboard.  \n \nFigure 9.1 conceptually illustrated a CPU -GPU  system with a dedicated GPU. A CPU has access to its \nown memory space (CPU RAM) and is  connected  to a GPU  via a PCI bus.  It is able  to send  data  and \ninstructions  over  the PCI bus for the GPU to work with. The GPU has its own memory space, separate \nfrom the CPU memory space. In  order for work to be executed on the GPU, at some point, data must \nbe transferred from the CPU to  the GPU. When the work is complete, and the results are going to be \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n3 \n written to file, the GPU must  send data back to the CPU. The instructions  the GPU must execute are \nalso sent from CPU to GPU.  Each  one of these  transactions  is mediated  by the PCI  bus.  \n \nThe GPU and the thread engine  \nThe thread engine within a CPU manages the execution of threads, schedules tasks for processing, and \nensures efficient utilization of available resources. The graphics processor is like the ideal thread engine.  \n\uf0d8 The components of this thread engine are  \n\uf0b7 A seemingly infinite number of threads  \n\uf0b7 Zero -time  cost for switching or starting threads:  refers to the ideal scenario where the process of \ninitiating or switching between threads occurs instantaneously, without any additional \ncomputational overhead.  \n\uf0b7 Latency  hiding of memory accesses through automatic switching between work groups : Memory \nlatency occurs because accessing data from the main memory (RAM) is significantly slower \ncompared to accessing data from the CPU's cache or registers . \n \n\uf0d8 For Example here we w ill  go through a single node system with a single multiprocessor CPU \nand two GPUs  \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n4 \n \uf0b7 Fig 9.2 Simplified Block Diagram of a GPU System consisting of two compute devices each \nhaving multiple compute units and separate GPU Memory.  \nA GPU is composed of  \n\uf0b7 Compute Device :  A compute device in a GPU is a subset of the GPU that is dedicated to \ngeneral -purpose parallel processing tasks. It consists of multiple compute units  \n\uf0b7 GPU RAM  : (also known as global memory)  refers to the dedicated memory that is integrated \ninto a graphics processing unit (GPU) or graphics card . \n\uf0b7 Workload distributor : Instructions and data received from the CPU are processed by the \nworkload distributor. The distributor coordinates instruction execution and data movement \nonto and off of the Compute Units.  \n\uf0b7 Compute units (CU ): Compute units are the fundamental processing units within a compute \ndevice. Each compute unit typically consists of multiple ALUs and multiple graphics \nprocessors called processing elements (PEs) . CUs have their own internal architecture, often \nreferred to as the microarchitecture . \n \n \n\uf0b7 Processing Element :  PEs within a GPU are designed to execute instructions in parallel. They \ncan handle multiple threads and data elements s imultaneously, allowing for massive \nparallelism.  \n \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n5 \n \uf0d8 Hardware Terminology  \n \nTable 9.1 summarizes the rough equivalence of terminology, in different har dware \narchitectures.  Example CPU in Host is termed as Compute Device (OpenCL ), GPU  in (AMD \nGPU) ,GPU  (NVIDIA/CUDA) and GPU in Intel Gen11.  \n \n\uf0d8 Calculating the peak theoretical flops for some leading GPUs  \n \nFLOPS provide a measure of a computer system's processing speed, especially when dealing with \nnumerical and scientific computations. It allows researchers  and developers to compare the \nperformance of different hardware architectures and configurations.  \nThe peak theoretical flops can be calculated by taking the clock rate times the number of processors \ntimes the number of floating -point operations per cycle.  The flops per cycle accounts for the fused -\nmultiply add (FMA), which does two operations in one cycle .  \nPeak Theoretical Flops (GFlops/s) = Clock rate MHZ \u00d7 Compute Units \u00d7 Processing units \u00d7 \nFlops/cycle . \n \nCharacteristics  of GPU  memory  spaces  \n \nGPU memory spaces are essential for efficient data management and processing in parallel \ncomputing environments, particularly in the context of graphics processing and high -performance \ncomputing.  Graphics Processing Units (GPUs) have multiple memory spaces with diffe rent \ncharacteristics, each optimized for specific types of tasks . \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n6 \n  \n \nFigure 9.4 Rectangles show each component of the GPU and the memory that is at each hardware \nlevel. The host writes and reads the global and constant memory. Each of the CUs can read and \nwrite from the global memory and read from the constant memory  \nThe list of the GPU memory types and their characteristics are  as follows.  \n\uf0b7 Private  memory  (register  memory) \u2014 Private memory in the context of GPUs typically refers \nto the local memory associated with individual processing element. It is  accessible  by a single  \nProcessing Element ( PE) and only  by that PE. \n\uf0b7 Local memory \u2014Accessible to a single C ontrol  Unit and all of the P rocessing Elements  on that \nControl Unit. \n \n\uf0b7 Constant  memory \u2014Constant memory is read -only, meaning that data stored in constant \nmemory cannot be modified by the GPU kernel during execution. It is primarily designed for \nread operations and is well-suited for storing constant values, lookup tables, or other data \nthat does not change during the kernel's execution  \n\uf0b7 Global memory \u2014Memory that \u2019s located on the GPU and accessible by all of the C ontrol \nUnit\u2019s \n \nThe performance of GPU memory, also known as memory subsystem performance, is a critical \nfactor that significantly impacts the overall performance of GPU -accelerated applications.  \n \n \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n7 \n  GPU memory performance is influenced by Memory Bandwidth  which can be calculated by \n \n \n \n \n \n  \n \nTheoretical Calculation:  Theoretical Bandwidth  = Memory Transaction Rate(Gbps) \u00d7 Memory \nbus (bits) \u00d7 (1 byte/8 bits)  \n \nGPU Stream Bench Mark:  In the context of GPUs (Graphics Processing Units) and general \ncomputing, a benchmark refers to a standardized test or set of tests designed to measure the \nperformance of a GPU or an entire computer system. Benchmarks are used to evaluate various \naspects of a GPU's capabilities, such as computational power , memory bandwidth, and graphics \nrendering performance.  For Example, the Babel STREAM Benchmark code measures the \nbandwidth of a variety of hardware with different programming languages.  \nThe mixbench tool  was developed to draw out the differences between the performance of \ndifferent GPU devices. Using the mixbench performance tool we can ch oose the best GPU for a \nworkload . \n \n Roofline Performance Model : The Roofline Performance Model is a graphical representation used \nin high -performance computing (HPC) to analyze and visualize the performance of algorithms on a \nTheoritical \nCalculation  GPU Stream \nBenchmark  Roofline Performance \nModel  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n8 \n particular hardware architecture . The  Roofline model serves as a visual communication tool that can \nbe easily shared among team members, researchers, and stakeholders. It provides a clear and \nconcise representation of performance characteristics, making it easier to convey insights and \noptimi zation recommendations.  \n \nThe PCI bus: CPU to GPU data transfer overhead  \nThe PCI (Peripheral Component Interconnect) bus is a standard interface that connects various \nhardware devices, including GPUs (Graphics Processing Units), to a computer's motherboard.  The \ncurrent version of the PCI bus is called PCI Express (PCIe).  \n \n \n \nThe Datatransfer includes the following steps for a typical program execeution on GPU  \n1. Copy data to GPU memory  \n2. CPU instructs the GPU (kernel configuration and launching)  \n3. Data processed by many cores in parallel  \n4. Copy result back to main memory.  \nWhen transferring data from the CPU to the GPU or vice versa over the PCI bus, there can be \noverhead associated with the data transfer process. This overhead is influenced by bandwidth  and \nMemory . \n            Bandwidth of PCI                                                                                                   Memory  \n \n \n \nEstimated : \n\uf0b7 theoretical peak \nperformance \nmodel  \n\uf0b7 micro -benchmark \napplication  Calculated : \n\uf0b7 Theoretical \nbandwidth of \nthe PCI bus  Pinned \nMemory  Pageable \nMemory  KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n9 \n  \nA back -of-the-envelope theoretical peak performance model:  The Theoretical Peak Performance \nModel is a concept used in high -performance computing to estimate the maximum computational \nperformance that a system or a specific hardware component can achieve under ideal conditions. \nThis model provides an upper bound on performance based on the theoretical capabilities of the \nhardware and is often used as a reference point for evaluating the efficiency of algorithms and \napplications.  \n A micro -benchmark application : A microbenchmark is a small, focused benchmark designed to \nmeasure the performance of a specific aspect of a system, component, or function. \nMicrobenchmarks are valua ble for isolating and evaluating the performance of a particular piece of \ncode or hardware, helping developers optimize and understand the efficiency of specific operations. \nThese benchmarks are often used during the development and testing phases to identi fy bottlenecks \nand make targeted improvements.  \nTheoretical bandwidth of the PCI bus (Calculated) : \nTheoretical Bandwidth (GB/s) = PCIe Lanes \u00d7 TransferRate (GT/s) \u00d7 OverheadFactor(Gb/GT) \u00d7 byte/8 \nbits \n \n\uf0b7 PCIe lanes refer to the individual data transfer paths within a PCI Express (PCIe) interface. \nEach lane is a point -to-point connection between two components, typically between a \ndevice (e.g., a graphics card, storage device) and the computer's motherboard . The concept \nof lanes is fundamental to understanding the bandwidth and data transfer capabilities of the \nPCIe . \n\uf0b7 The maximum transfer rates for each lane in a PCIe bus can directly be determined by its \ndesign generation. Generation is a specification for the required performance of the \nhardware, much like 4G is an industry standard for cell -phones. The PCI Special Interest \nGroup (PCI SIG) represents industry partners and establishes a PCIe specification that is \ncommonly referred to as generation or gen for  short . \n\uf0b7 Transmitting data across the PCI bus requires additional overhead. Generation 1 and 2 \nstandards stipulate that 10 bytes are transmitted for every 8 bytes of useful data. Starting \nwith generation 3, the transfer transmits 130 bytes for every 128 bytes of data. The overhead \nfactor is the ratio of the number of usable bytes over the total bytes transmitted . KR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n10 \n  \nPCI Express (PCIe) specifications by generation  \n \nPinned memory,  also known as locked or page -locked memory, is a type of memory in a computer \nsystem that remains fixed in physical RAM and is not subject to swapping to disk by the operating \nsystem's virtual memory manager. In GPU programming, pinned memory is often used to facilitate \nfast data transfers between the CPU and GPU. Pinned memory cons umes physical RAM exclusively. \nApplications using a significant amount of pinned memory may impact overall system resources, so \ncareful consideration is needed to avoid resource exhaustion.  \nPageable memory , also known as virtual memory, is a type of memory  management strategy used \nby operating systems to efficiently handle the allocation and deallocation of memory for running \nprocesses. In a pageable memory system, the operating system divides the physical memory into \nfixed -size blocks called pages. These p ages can be dynamically moved between the computer's RAM \n(Random Access Memory) and a secondary storage device, such as a hard disk, to optimize overall \nsystem performance.  \n \nMulti -GPU platforms and MPI  \nA multi -GPU (Graphics Processing Unit) platform refers  to a system configuration that incorporates \nmore than one GPU. Multi -GPU setups are commonly used in high -performance computing, \nscientific simulations, and graphics -intensive applications to enhance computational power and \ngraphics rendering capabilities . \n \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n11 \n  \nTo use multiple GPUs, we have to send data from one GPU to another.  \n\uf0d8 Standard data transfer process : This has a lot of data movement and will be a major limitation \nto application performance.  \n \n1 Copy the data from the GPU to the host processor  \n a Move the data across the PCI bus to the processor  \n b Store the data in CPU DRAM memory  \n2 Send the data in an MPI message to another processor  \na Stage the data from CPU memory to the processor  \nb Move the data across the PCI bus to the network interfac e card (NIC)  \nc Store the data from the processor to CPU memory  \n3 Copy the data from the second processor to the second GPU  \n a Load the data from CPU memory to the processor  \n b Send the data across the PCI bus to the GPU  \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n12 \n \uf0d8 Optimizing the data movement between GPUs across the network : The data movement \nbypasses the CPU when moving data from one GPU to another  \n \n \n Potential benefits of GPU -accelerated platforms  \n1. Reducing time -to-solution  \nReducing time -to-solution in GPU (Graphics Processing Unit) computing  involves optimizing the \ncode, leveraging parallel processing capabilities, and making efficient use of GPU resources . \nStructure the computations to take advantage of data parallelism, where the same operation is \nperformed on multiple data elements concurrently. This aligns well with the architecture of \nGPUs, which excel at handling parallel tasks.  \n2. Reducing energy use with GPUs  \nReducing energy use with GPUs involves optimizing your GPU -accelerated applications to \nachieve computational efficiency while considering power consumption.  \nThe energy consumption for your application can be estimated using the formula  \nEnergy = (N Processors) \u00d7 (R Watts/Processor) \u00d7 (T hours)  \nAchieving a reduction in energy cost through GPU accelerator devices requires th at the \napplication expose sufficient parallelism and that the device \u2019s resources are efficiently utilized.  \n3. Reduction in cloud computing costs with GPUs  \nCloud computing services from Google and Amazon let you match your workloads to a wide range \nof compute  server types and demands.  \nKR21  PARALLEL PROGRAMMING  CSE/IT/CSM/CSD  \n13 \n \uf0b7 If your application is memory bound, you can use a GPU that has a lower flops to -loads ratio \nat a lower cost.  \n\uf0b7 If you are more concerned with turnaround time, you can add more GPUs or CPUs.  \n\uf0b7 If your deadlines are less serious, y ou can use preemptible resources at a considerable \nreduction in cost.  \nAs the cost of computing is more visible with cloud computing services, optimizing application \u2019s \nperformance becomes a higher priority. Cloud computing has the advantage of giving you access \nto a wider variety of hardware than you can have on -site and more options to match the hardware \nto the workload.  \nWhen to use GPUs  \nGPUs are not general -purpose p rocessors. They are most appropriate when the computation \nworkload is similar to a graphics workload \u2014lots of operations that are identical.  \nThere are some areas where GPUs still do not perform well . \n\uf0b7 Lack of parallelism \u2014 \u201cWith great power comes great need for parallelism. \u201d If you don \u2019t have \nthe parallelism, GPUs can \u2019t do a lot for you. This is the first law of GPGPU programming.  \n\uf0b7 Irregular memory access \u2014CPUs also struggle with this. The massive parallelism of GPUs \nbrings no benefit to this situation.  \n\uf0b7 Dynami c memory requirements \u2014Memory allocation is done on the CPU, which severely \nlimits algorithms that require memory sizes determined on the fly.  \n\uf0b7 Recursive algorithms \u2014GPUs have limited stack memory resources, and suppliers often state \nthat recursion is not su pported.  \n"
}